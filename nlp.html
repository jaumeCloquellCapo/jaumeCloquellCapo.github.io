<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Topic modelling - jaumecloquellcapo</title><meta name="description" content=" In the next section, we provide a detailed description for the above mentioned step We use the following function to read the data as a dataframe. The unzipped file is quite large. It has almost 1 GB of pure plain text (that's about 120 thousand&hellip;"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://jaumeCloquellCapo.github.io/jaumeCloquellCapo.github.io/nlp.html"><link rel="amphtml" href="https://jaumeCloquellCapo.github.io/jaumeCloquellCapo.github.io/amp/nlp.html"><link rel="alternate" type="application/atom+xml" href="https://jaumeCloquellCapo.github.io/jaumeCloquellCapo.github.io/feed.xml"><link rel="alternate" type="application/json" href="https://jaumeCloquellCapo.github.io/jaumeCloquellCapo.github.io/feed.json"><meta property="og:title" content="Topic modelling"><meta property="og:site_name" content="jaumecloquellcapo"><meta property="og:description" content=" In the next section, we provide a detailed description for the above mentioned step We use the following function to read the data as a dataframe. The unzipped file is quite large. It has almost 1 GB of pure plain text (that's about 120 thousand&hellip;"><meta property="og:url" content="https://jaumeCloquellCapo.github.io/jaumeCloquellCapo.github.io/nlp.html"><meta property="og:type" content="article"><link rel="shortcut icon" href="https://jaumeCloquellCapo.github.io/jaumeCloquellCapo.github.io/media/website/favicon.ico" type="image/x-icon"><style>:root{--body-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--heading-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--logo-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--menu-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol"}</style><link rel="stylesheet" href="https://jaumeCloquellCapo.github.io/jaumeCloquellCapo.github.io/assets/css/style.css?v=47f8a4f068382085929af7e1db2c0ae5"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://jaumeCloquellCapo.github.io/jaumeCloquellCapo.github.io/nlp.html"},"headline":"Topic modelling","datePublished":"2020-03-19T17:42","dateModified":"2020-03-24T18:44","description":" In the next section, we provide a detailed description for the above mentioned step We use the following function to read the data as a dataframe. The unzipped file is quite large. It has almost 1 GB of pure plain text (that's about 120 thousand&hellip;","author":{"@type":"Person","name":"jaume cloquell capo"},"publisher":{"@type":"Organization","name":"jaume cloquell capo"}}</script></head><body><div class="site-container"><header class="top" id="js-header"><a class="logo" href="https://jaumeCloquellCapo.github.io/jaumeCloquellCapo.github.io/">jaumecloquellcapo</a><nav class="navbar js-navbar"><button class="navbar__toggle js-toggle" aria-label="Menu" aria-haspopup="true" aria-expanded="false"><span class="navbar__toggle-box"><span class="navbar__toggle-inner">Menu</span></span></button><ul class="navbar__menu"><li><a href="https://jaumeCloquellCapo.github.io/jaumeCloquellCapo.github.io/" title="https://jaumecloquellcapo.github.io/jaumeCloquellCapo.github.io/" target="_self">Home</a></li><li><a href="https://jaumeCloquellCapo.github.io/jaumeCloquellCapo.github.io/js/" target="_self">JS</a></li><li class="has-submenu"><a href="https://jaumeCloquellCapo.github.io/jaumeCloquellCapo.github.io/machine-learning/" target="_self" aria-haspopup="true">AI</a><ul class="navbar__submenu level-2" aria-hidden="true"><li><a href="https://jaumeCloquellCapo.github.io/jaumeCloquellCapo.github.io/abm/" target="_self">ABM</a></li><li><a href="https://jaumeCloquellCapo.github.io/jaumeCloquellCapo.github.io/nlp/" target="_self">NLP</a></li><li><a href="https://jaumeCloquellCapo.github.io/jaumeCloquellCapo.github.io/geneticalgorithms/" target="_self">Genetic Algorithms</a></li></ul></li><li><a href="https://jaumeCloquellCapo.github.io/jaumeCloquellCapo.github.io/golang/" target="_self">Golang</a></li></ul></nav><div class="search"><div class="search__overlay js-search-overlay"><div class="search__overlay-inner"><form action="https://jaumeCloquellCapo.github.io/jaumeCloquellCapo.github.io/search.html" class="search__form"><input class="search__input js-search-input" type="search" name="q" placeholder="search..." aria-label="search..." autofocus="autofocus"></form><button class="search__close js-search-close" aria-label="Close">Close</button></div></div><button class="search__btn js-search-btn" aria-label="Search"><svg role="presentation" focusable="false"><use xlink:href="https://jaumeCloquellCapo.github.io/jaumeCloquellCapo.github.io/assets/svg/svg-map.svg#search"/></svg></button></div></header><main><article class="post"><div class="hero"><figure class="hero__image hero__image--overlay"><img src="https://jaumeCloquellCapo.github.io/jaumeCloquellCapo.github.io/media/website/82-820639_big-dark-mountain-hd-wallpaper-1920x1280p-martin-garrix.jpg" srcset="https://jaumeCloquellCapo.github.io/jaumeCloquellCapo.github.io/media/website/responsive/82-820639_big-dark-mountain-hd-wallpaper-1920x1280p-martin-garrix-xs.jpg 300w, https://jaumeCloquellCapo.github.io/jaumeCloquellCapo.github.io/media/website/responsive/82-820639_big-dark-mountain-hd-wallpaper-1920x1280p-martin-garrix-sm.jpg 480w, https://jaumeCloquellCapo.github.io/jaumeCloquellCapo.github.io/media/website/responsive/82-820639_big-dark-mountain-hd-wallpaper-1920x1280p-martin-garrix-md.jpg 768w, https://jaumeCloquellCapo.github.io/jaumeCloquellCapo.github.io/media/website/responsive/82-820639_big-dark-mountain-hd-wallpaper-1920x1280p-martin-garrix-lg.jpg 1024w, https://jaumeCloquellCapo.github.io/jaumeCloquellCapo.github.io/media/website/responsive/82-820639_big-dark-mountain-hd-wallpaper-1920x1280p-martin-garrix-xl.jpg 1360w, https://jaumeCloquellCapo.github.io/jaumeCloquellCapo.github.io/media/website/responsive/82-820639_big-dark-mountain-hd-wallpaper-1920x1280p-martin-garrix-2xl.jpg 1600w" sizes="(max-width: 1600px) 100vw, 1600px" loading="eager" alt=""></figure><header class="hero__content"><div class="wrapper"><div class="post__meta"><time datetime="2020-03-19T17:42">March 19, 2020</time></div><h1>Topic modelling</h1><div class="post__meta post__meta--author"><a href="https://jaumeCloquellCapo.github.io/jaumeCloquellCapo.github.io/authors/jaume-cloquell-capo/" class="feed__author invert">jaume cloquell capo</a></div></div></header></div><div class="wrapper post__entry"><p> </p><div class="t m0 x1 h5 y5 ff4 fs4 fc0 sc0 ls1 ws8"><a href="https://www.researchgate.net/publication/335402798_Research_paper_classification_systems_based_on_TF-IDF_and_LDA_schemes">https://www.researchgate.net/publication/335402798_Research_paper_classification_systems_based_on_TF-IDF_and_LDA_schemes</a></div><div class="t m0 x1 h5 y5 ff4 fs4 fc0 sc0 ls1 ws8"> </div><div class="t m0 x1 h5 y5 ff4 fs4 fc0 sc0 ls1 ws8">https://medium.com/@crscardellino/procesando-datos-con-spark-y-iv-corriendo-una-aplicaci%C3%B3n-con-pyspark-5c26e828465d</div><div class="t m0 x1 h5 y5 ff4 fs4 fc0 sc0 ls1 ws8"> </div><div class="t m0 x1 h5 y5 ff4 fs4 fc0 sc0 ls1 ws8"><a href="https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/">https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/</a></div><div class="t m0 x1 h5 y5 ff4 fs4 fc0 sc0 ls1 ws8"> </div><div class="t m0 x1 h5 y5 ff4 fs4 fc0 sc0 ls1 ws8"> </div><div class="t m0 x1 h5 y5 ff4 fs4 fc0 sc0 ls1 ws8">Numerous research papers have been published online as well as oﬄine with the</div><div class="t m0 x1 h5 y6 ff4 fs4 fc0 sc0 ls1 ws9">increasing advance of computer and information technologies, which makes it diﬃcult</div><div class="t m0 x1 h5 y7 ff4 fs4 fc0 sc0 ls1 wsa">for users to search and categorize their interesting research papers for a speciﬁc subject.</div><div class="t m0 x1 h5 y7 ff4 fs4 fc0 sc0 ls1 wsa"> </div><div class="t m2 x7 h5 y41 ff4 fs4 fc0 sc0 ls1 ws23">To classify a huge number of papers into papers with similar subjects, we propose</div><div class="t m2 x6 h5 y42 ff4 fs4 fc0 sc0 ls1 ws24">the paper classiﬁcation system based on term frequency-inverse document frequency</div><div class="t m2 x6 h11 y43 ff4 fs4 fc0 sc0 ls1 ws25">(TF-IDF) and Latent Dirichlet allocation (LDA)  schemes. The proposed system ﬁrstly constructs a representative keyword dictionary with the keywords that user</div><div class="t m2 x6 h5 y45 ff4 fs4 fc0 sc0 ls1 ws27">inputs, and with the topics extracted by the LDA. Secondly, it uses the TF-IDF scheme</div><div class="t m2 x6 h5 y46 ff4 fs4 fc0 sc0 ls1 ws28">to extract subject words from the abstract of papers based on the keyword dictionary.</div><div class="t m2 x6 h12 y47 ff4 fs4 fc0 sc0 ls1 ws29">Then, the K-means clustering algorithm is applied to classify the papers with simi<span class="v0">-</span></div><div class="t m2 x6 h5 y48 ff4 fs4 fc0 sc0 ls1 ws7">lar subjects, based on the TF-IDF values of each document.</div><div class="t m2 x6 h5 y48 ff4 fs4 fc0 sc0 ls1 ws7"> </div><div class="t m2 x6 h5 y48 ff4 fs4 fc0 sc0 ls1 ws7"><div class="t m2 x7 h5 y49 ff4 fs4 fc0 sc0 ls1 ws2a">To extract subject words from a set of massive papers eﬃciently, in this paper, we use</div><div class="t m2 x6 h5 y4a ff4 fs4 fc0 sc0 ls1 wsc">the Spark that can process big data rapidly and stably with high scalability.</div><div class="t m2 x6 h5 y4a ff4 fs4 fc0 sc0 ls1 wsc"> </div><div class="t m2 x6 h5 y4a ff4 fs4 fc0 sc0 ls1 wsc"><strong>System flow diagram</strong></div><div class="t m2 x6 h5 y4a ff4 fs4 fc0 sc0 ls1 wsc"> </div><div class="t m2 x6 h5 y4a ff4 fs4 fc0 sc0 ls1 wsc">Thee paper classiﬁcation system proposed in this post consists of three main processes<br><ul><li class="t m2 x6 h5 y9a ff4 fs4 fc0 sc0 ls1 ws60">Step 1. Read and preprocessing the data<ul><li class="t m2 x6 h5 y9a ff4 fs4 fc0 sc0 ls1 ws60">I executes preprocessing for the data, such as the removal of stop words, the extraction of only nouns, etc.</li><li class="t m2 x6 h5 y9d ff4 fs4 fc0 sc0 ls1 ws1e">It constructs a keyword dictionary based on crawled keywords. Because total keywords of whole papers are huge, this paper uses only top-N keywords with high frequency among the whole keywords.</li></ul></li><li class="t m2 x6 h5 ya0 ff4 fs4 fc0 sc0 ls1 ws7">Step 2. Extract Topics<ul><li class="t m2 x6 h5 ya0 ff4 fs4 fc0 sc0 ls1 ws7">It extracts topics from the crawled abstracts by LDA topic modeling</li><li class="t m2 x6 h5 ya1 ff4 fs4 fc0 sc0 ls1 ws62">It calculates paper lengths as the number of occurrences of words in the abstract of each paper</li><li class="t m2 x6 h5 ya1 ff4 fs4 fc0 sc0 ls1 ws62">It calculates a TF value for both of the keywords obtained by Step 2 and the topics obtained by Step 3</li><li class="t m2 x6 h5 ya1 ff4 fs4 fc0 sc0 ls1 ws62">It calculates an IDF value for both of the keywords obtained by Step 2 and the topics obtained by Step 3.</li><li class="t m2 x6 h5 ya1 ff4 fs4 fc0 sc0 ls1 ws62">It calculates a TF-IDF value for each keyword using the values obtained by Steps 4, 5, and 6</li></ul></li></ul><ul><li class="t m2 x6 h5 ya9 ff4 fs4 fc0 sc0 ls1 ws64">Step 8. Group by topics <ul><li class="t m2 x6 h5 ya9 ff4 fs4 fc0 sc0 ls1 ws64">It groups the whole papers into papers with a similar subject, based on the K-means clustering algorithm In the next section, we provide a detailed description for the above mentioned steps</li></ul></li></ul><p> </p></div></div><p>In the next section, we provide a detailed description for the above mentioned step</p><h3>STEP 1: PREPROCESSING THE DATA</h3><p>We use the following function to read the data as a dataframe. The unzipped file is quite large. It has almost 1 GB of pure plain text (that's about 120 thousand documents).</p><pre>from pyspark.ml.feature import RegexTokenizer<br>from pyspark.ml.feature import RegexTokenizer<br>from pyspark.ml.feature import StopWordsRemover<br>from pyspark.ml.feature import CountVectorizer<br>from pyspark.ml.feature import IDF<br>from pyspark.sql.functions import monotonically_increasing_id<br>from nltk.stem.snowball import SnowballStemmer<br>from pyspark.sql.functions import udf<br>from pyspark.sql.types import ArrayType, FloatType, StringType<br>from pyspark.sql.functions import udf, lit<br><br>data = spark.read.text("dataset/infoleg.txt")</pre><p data-selectable-paragraph="">First I select the column which has the reviews and remove all the empty rows. Then we can use the regular expression [^\p{L}] which will separate the tokens by characters other than English letters (e.g. spaces, numbers, punctuation marks, etc.). Regarding this point, I have a recommendation: always work with the UTF-8 Unicode encoding for Spanish, you will save a lot of problems (check out the iconv for conversion between UTF-8 and ISO-8859-1, which is the other encoding widely used for Spanish files). </p><pre>tokenizer = RegexTokenizer(inputCol="value", outputCol="words", pattern=r"[^\p{L}]+")<br>remover = StopWordsRemover(stopWords=StopWordsRemover.loadDefaultStopWords("spanish"), inputCol="words", outputCol="tokens")<br>tokenizedLaw = remover.transform(tokenizer.transform(data))</pre><p>Then I remove all the extra spaces between words, split each document into list of words, change them into lowercase, check if they’re alpha numeric, remove any words or typos which are less than two letters, remove any stopwords, and finally apply Stemming algorithm. This<span id="hs_cos_wrapper_post_body" class="hs_cos_wrapper hs_cos_wrapper_meta_field hs_cos_wrapper_type_rich_text" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text"> algorithms work by cutting off the end or the beginning of the word, taking into account a list of common prefixes and suffixes that can be found in an inflected word.</span></p><pre><br>stemmer = SnowballStemmer(language='spanish')<br><br>def stem (tokens):<br> return [stemmer.stem(word) for word in tokens if len(word) &gt; 2]<br><br>def alphaNumeric (tokens):<br> return [re.sub('\d',' ', word) for word in tokens]<br><br>udf_alphaNumeric = udf(alphaNumeric, ArrayType(StringType()))<br>udf_stem = udf(stem, ArrayType(StringType())) #Define UDF function<br><br>alphaNumericLaw = tokenizedLaw.withColumn('alphaNumeric_tokens',udf_alphaNumeric('tokens'))<br>lemmatizedLaw = alphaNumericLaw.withColumn('lemmatized_tokens',udf_stem('alphaNumeric_tokens'))</pre><h3>STEP 1: EXTRACT TOPICS</h3><p>Next Im going to build a TF-IDF matrix before I run the LDA. In Spark MLlib, TF and IDF are implemented separately.</p><ul><li>Term frequency vectors could be generated using HashingTF or CountVectorizer.</li><li>IDF is an Estimator which is fit on a dataset and produces an IDFModel. The IDFModel takes feature vectors (generally created from HashingTF or CountVectorizer) and scales each column. Intuitively, it down-weights columns which appear frequently in a corpus.</li></ul><p> </p><figure class="post__image post__image--center"><img loading="lazy" src="https://www.tutorialkart.com/wp-content/uploads/2018/02/tfidf_flow.png" data-is-external-image="true" width="559" height="552"></figure><p> </p><pre class="lang-py prettyprint prettyprinted"><code><span class="pln"># Example<br></span>
<span class="pun">+-----+---------------+-------------------------+</span>
<span class="pun">|</span><span class="pln">label</span><span class="pun">|</span><span class="pln">raw            </span><span class="pun">|</span><span class="pln">vectors                  </span><span class="pun">|</span>
<span class="pun">+-----+---------------+-------------------------+</span>
<span class="pun">|</span><span class="lit">0</span>    <span class="pun">|[</span><span class="pln">a</span><span class="pun">,</span><span class="pln"> b</span><span class="pun">,</span><span class="pln"> c</span><span class="pun">]</span>      <span class="pun">|(</span><span class="lit">3</span><span class="pun">,[</span><span class="lit">0</span><span class="pun">,</span><span class="lit">1</span><span class="pun">,</span><span class="lit">2</span><span class="pun">],[</span><span class="lit">1.0</span><span class="pun">,</span><span class="lit">1.0</span><span class="pun">,</span><span class="lit">1.0</span><span class="pun">])|</span>
<span class="pun">|</span><span class="lit">1</span>    <span class="pun">|[</span><span class="pln">a</span><span class="pun">,</span><span class="pln"> b</span><span class="pun">,</span><span class="pln"> b</span><span class="pun">,</span><span class="pln"> c</span><span class="pun">,</span><span class="pln"> a</span><span class="pun">]|(</span><span class="lit">3</span><span class="pun">,[</span><span class="lit">0</span><span class="pun">,</span><span class="lit">1</span><span class="pun">,</span><span class="lit">2</span><span class="pun">],[</span><span class="lit">2.0</span><span class="pun">,</span><span class="lit">2.0</span><span class="pun">,</span><span class="lit">1.0</span><span class="pun">])|</span>
<span class="pun">+-----+---------------+-------------------------+</span></code></pre><p> </p><pre><code># Utilizaremos la clase CountVectorizer, creamos un contador y transformamos el conjunto de datos<br><br></code><br><code>counter = CountVectorizer(inputCol="lemmatized_tokens", outputCol="term_frequency", minDF=5)</code><br><code>counterModel = counter.fit(lemmatizedLaw)</code><br><code>vectorizedLaw = counterModel.transform(lemmatizedLaw)<br><br></code><br><code># Aplicamos IDF sobre el conteo de palabras</code><br><code>idf = IDF(inputCol="term_frequency", outputCol="tf_idf")</code><br><code>tfidfLaw = idf.fit(vectorizedLaw).transform(vectorizedLaw)</code></pre></div><footer class="wrapper post__footer"><p class="post__last-updated">This article was updated on March 24, 2020</p><ul class="post__tag"><li><a href="https://jaumeCloquellCapo.github.io/jaumeCloquellCapo.github.io/nlp/">Natural language processing</a></li><li><a href="https://jaumeCloquellCapo.github.io/jaumeCloquellCapo.github.io/spark/">Spark</a></li></ul><div class="post__share"><a href="http://www.linkedin.com/shareArticle?url=https%3A%2F%2FjaumeCloquellCapo.github.io%2FjaumeCloquellCapo.github.io%2Fnlp.html&amp;title=Topic%20modelling" class="js-share linkedin" rel="nofollow noopener noreferrer"><svg class="icon" aria-hidden="true" focusable="false"><use xlink:href="https://jaumeCloquellCapo.github.io/jaumeCloquellCapo.github.io/assets/svg/svg-map.svg#linkedin"/></svg> <span>LinkedIn</span></a></div><div class="post__bio bio"><div class="bio__info"><h3 class="bio__name"><a href="https://jaumeCloquellCapo.github.io/jaumeCloquellCapo.github.io/authors/jaume-cloquell-capo/" class="invert" rel="author">jaume cloquell capo</a></h3></div></div></footer></article><div class="post__comments"><div class="wrapper"><h2 class="h5">Comments</h2><div id="disqus_thread"></div><script>var disqus_config = function () {
                       this.page.url = 'https://jaumeCloquellCapo.github.io/jaumeCloquellCapo.github.io/nlp.html';
               		this.page.identifier = '13';
                   };
               
                   var disqus_loaded = false;
               
                   function publiiLoadDisqus() {
                       if(disqus_loaded) {
                           return false;
                       }
               
                       var top = document.getElementById('disqus_thread').offsetTop;
               
                       if (!disqus_loaded && (window.scrollY || window.pageYOffset) + window.innerHeight > top) {
                           disqus_loaded = true;
               
                           (function () {
                               var d = document, s = d.createElement('script');
                               s.src = 'https://'+'https-jaumecloquellcapo-github-io'.trim()+'.disqus.com/embed.js';
                               s.setAttribute('data-timestamp', +new Date());
                               (d.head || d.body).appendChild(s);
                           })();
                       }
                   }
               
                   publiiLoadDisqus();
               
                   window.onscroll = function() {
                       publiiLoadDisqus();
                   };</script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" target="_blank" rel="nofollow noopener noreferrer">comments powered by Disqus.</a></noscript></div></div></main><footer class="footer"><div class="footer__social"><a href="https://www.linkedin.com/in/jaume-cloquell-119765b7" aria-label="LinkedIn"><svg><use xlink:href="https://jaumeCloquellCapo.github.io/jaumeCloquellCapo.github.io/assets/svg/svg-map.svg#linkedin"/></svg></a></div><div class="footer__copyright"><p>jaume cloquell capo</p></div><button class="footer__bttop js-footer__bttop" aria-label="Back to top"><svg><title>Back to top</title><use xlink:href="https://jaumeCloquellCapo.github.io/jaumeCloquellCapo.github.io/assets/svg/svg-map.svg#toparrow"/></svg></button></footer></div><script>window.publiiThemeMenuConfig = {    
        mobileMenuMode: 'overlay',
        animationSpeed: 300,
        submenuWidth: 'auto',
        doubleClickTime: 500,
        mobileMenuExpandableSubmenus: true, 
        relatedContainerForOverlayMenuSelector: '.top',
   };</script><script defer="defer" src="https://jaumeCloquellCapo.github.io/jaumeCloquellCapo.github.io/assets/js/scripts.min.js?v=f4c4d35432d0e17d212f2fae4e0f8247"></script><script>var images = document.querySelectorAll('img[loading]');

        for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
                images[i].classList.add('is-loaded');
            } else {
                images[i].addEventListener('load', function () {
                    this.classList.add('is-loaded');
                }, false);
            }
        }</script><div class="cookie-popup js-cookie-popup cookie-popup--uses-badge"><h2>This website uses cookies</h2><p>Select which cookies to opt-in to via the checkboxes below; our website uses cookies to examine site traffic and user activity while on our site, for marketing, and to provide social media functionality. <a href="#not-specified">More details...</a></p><form><input id="gdpr-necessary" name="gdpr-necessary" checked="checked" disabled="disabled" type="checkbox"> <label for="gdpr-necessary">Required</label> <input id="gdpr-functions" name="gdpr-functions" type="checkbox"> <label for="gdpr-functions">Functionality</label> <input id="gdpr-analytics" name="gdpr-analytics" type="checkbox"> <label for="gdpr-analytics">Analytical</label> <input id="gdpr-marketing" name="gdpr-marketing" type="checkbox"> <label for="gdpr-marketing">Marketing</label><p class="cookie-popup__save-wrapper"><button type="submit" class="cookie-popup__save">Save</button></p></form><span class="cookie-popup-label">Cookie Policy</span></div><script>(function() {
                function addScript (src, inline) {
                    var newScript = document.createElement('script');

                    if (src) {
                        newScript.setAttribute('src', src);
                    }

                    if (inline) {
                        newScript.text = inline;
                    }

                    document.body.appendChild(newScript);
                }

                var popup = document.querySelector('.js-cookie-popup');
                var checkboxes = popup.querySelectorAll('input[type="checkbox"]');
                var save = popup.querySelector('button');
                var currentConfig = localStorage.getItem('publii-gdpr-allowed-cookies');
                var blockedScripts = document.querySelectorAll('script[type^="gdpr-blocker/"]');
                

                popup.addEventListener('click', function() {
                    if (!popup.classList.contains('cookie-popup--is-sticky')) {
                        popup.classList.add('cookie-popup--is-sticky');
                    }
                });

                save.addEventListener('click', function(e) {
                    e.preventDefault();
                    e.stopPropagation();
                    popup.classList.remove('cookie-popup--is-sticky');
                    var allowedGroups = [];

                    for (var i = 0; i < checkboxes.length; i++) {
                        if (checkboxes[i].checked) {
                            var groupName = checkboxes[i].getAttribute('name').replace('gdpr-', '');
                            var scripts = document.querySelectorAll('script[type="gdpr-blocker/' + groupName + '"]');

                            for (var j = 0; j < scripts.length; j++) {
                                addScript(scripts[j].src, scripts[j].text);
                            }

                            allowedGroups.push(groupName);
                        }
                    }

                    localStorage.setItem('publii-gdpr-allowed-cookies', allowedGroups.join(','));
                    popup.classList.remove('cookie-popup--is-sticky');

                    setTimeout(function () {
                        if (currentConfig !== null) {
                            window.location.reload();
                        }
                    }, 250);
                });

                if (currentConfig === null) {
                    popup.classList.add('cookie-popup--is-sticky');
                    var checkedGroups = popup.querySelectorAll('input[type="checkbox"]:checked');

                    for (var i = 0; i < checkedGroups.length; i++) {
                        var allowedGroup = checkedGroups[i].name.replace('gdpr-', '');

                        if (allowedGroup !== '-' && allowedGroup !== '') {
                            var scripts = document.querySelectorAll('script[type="gdpr-blocker/' + allowedGroup + '"]');

                            for (var j = 0; j < scripts.length; j++) {
                                addScript(scripts[j].src, scripts[j].text);
                            }
                        }
                    }
                } else {
                    if (currentConfig !== '') {
                        var allowedGroups = currentConfig.split(',');
                        var checkedCheckboxes = popup.querySelectorAll('input[type="checkbox"]:checked');

                        for (var j = 0; j < checkedCheckboxes.length; j++) {
                            var name = checkedCheckboxes[j].name.replace('gdpr-', '');

                            if (allowedGroups.indexOf(name) === -1) {
                                checkedCheckboxes[j].checked = false;
                            }
                        }

                        for (var i = 0; i < allowedGroups.length; i++) {
                            var scripts = document.querySelectorAll('script[type="gdpr-blocker/' + allowedGroups[i] + '"]');
                            var checkbox = popup.querySelector('input[type="checkbox"][name="gdpr-' + allowedGroups[i] + '"]');

                            if (checkbox) {
                                checkbox.checked = true;
                            }

                            for (var j = 0; j < scripts.length; j++) {
                                addScript(scripts[j].src, scripts[j].text);
                            }
                        }
                    }
                }
            })();</script></body></html>